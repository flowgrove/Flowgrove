#!/usr/bin/env python3
"""
kn_ai_final_dropin.py â€” Final condensed autonomous drop-in

Single-file. Condensed, safe-by-default, all-features-available behind env opt-in.

ENVs (all optional; defaults are safe/off):
  KN_ALLOW_NETWORK=1           # enable network features (mirror, raw updates, Replit)
  KN_ALLOW_AUTO_PUSH=1        # enable automatic git push (opt-in)
  KN_GITHUB_TOKEN=ghp_xxx     # set as secret in environment (only if auto-push)
  KN_GITHUB_REPO=user/repo    # repo slug (only if auto-push)
  KN_MASTER_RAW_URL=...       # optional raw URL to pull master self-updates
  KN_WORKER_COUNT=N           # >0 to run manager mode spawning worker processes
  KN_MIN_MODULES, KN_MAX_MODULES   # self-balancing module limits
  KN_ALLOW_DANGEROUS=1        # enable dangerous shell (explicit opt-in)
  KN_REPLIT_BUCKET, KN_REPLIT_TOKEN # optional Replit object storage
  KN_NODE_ID=name             # optional worker id
Notes: Use env/secrets; do NOT edit tokens into this file.
"""

import os, sys, time, json, threading, subprocess, logging, random, shutil, argparse
from pathlib import Path
from datetime import datetime, timedelta

# ---------- Configuration & safety gates ----------
ROOT = Path(os.getenv("KN_ROOT", ".")).resolve()
os.chdir(ROOT)
LOG = ROOT / "kn_ai.log"
HEART = ROOT / ".kn_heart"; HEART.mkdir(exist_ok=True)
BACKUP = ROOT / "kn_backups"; BACKUP.mkdir(exist_ok=True)
INSTR = ROOT / ".kn_instr"; INSTR.mkdir(exist_ok=True)
PROCESSED = ROOT / ".kn_instr_done"; PROCESSED.mkdir(exist_ok=True)
MODULES = ROOT / "kn_modules"; MODULES.mkdir(exist_ok=True)

ALLOW_NETWORK = os.getenv("KN_ALLOW_NETWORK", "0") == "1"
ALLOW_AUTO_PUSH = os.getenv("KN_ALLOW_AUTO_PUSH", "0") == "1"
ALLOW_DANGEROUS = os.getenv("KN_ALLOW_DANGEROUS", "0") == "1"
GITHUB_TOKEN = os.getenv("KN_GITHUB_TOKEN")
GITHUB_REPO = os.getenv("KN_GITHUB_REPO")
MASTER_RAW_URL = os.getenv("KN_MASTER_RAW_URL")
WORKER_COUNT = int(os.getenv("KN_WORKER_COUNT", "0"))
MIN_MODULES = int(os.getenv("KN_MIN_MODULES", "4"))
MAX_MODULES = int(os.getenv("KN_MAX_MODULES", "28"))

# Defaults for runtime safety
HEARTBEAT_INTERVAL = float(os.getenv("KN_HEARTBEAT_SEC", "2"))
STALE_SEC = float(os.getenv("KN_STALE_SEC", "8"))
MAX_RESTARTS = int(os.getenv("KN_MAX_RESTARTS", "6"))

# ---------- Logging ----------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")
def log(msg):
    t = f"[{datetime.utcnow().isoformat()}] {msg}"
    print(t, flush=True)
    try:
        with open(LOG, "a", encoding="utf-8") as fh: fh.write(t + "\n")
    except Exception:
        pass

# ---------- Utilities ----------
def now(): return datetime.utcnow().isoformat()
def list_files(globs=("**/*.py","**/*.js","**/*.html","**/*.css","**/*.json","**/*.md")):
    out=[]
    for g in globs:
        for p in ROOT.glob(g):
            if any(x in p.parts for x in (".git","node_modules","__pycache__", "kn_backups")): continue
            if p.is_file(): out.append(p)
    return sorted(out)

def backup(p:Path):
    try:
        dst = BACKUP / p.relative_to(ROOT)
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(p, dst)
        log(f"backup:{p.relative_to(ROOT)}")
    except Exception as e:
        log(f"backup err {p}: {e}")

# ---------- Integrity ----------
INTEGRITY = ROOT / ".kn_integrity.json"
def build_index():
    idx={}
    for p in list_files():
        try:
            idx[str(p.relative_to(ROOT))] = {"size":p.stat().st_size, "mtime": p.stat().st_mtime}
        except: pass
    try: INTEGRITY.write_text(json.dumps(idx,indent=2), encoding="utf-8")
    except Exception: pass
    return idx

def integrity_check():
    if not INTEGRITY.exists(): return
    try:
        saved=json.loads(INTEGRITY.read_text(encoding="utf-8"))
        for p in list_files():
            rel=str(p.relative_to(ROOT))
            cur_mtime = p.stat().st_mtime
            old = saved.get(rel)
            if old and cur_mtime != old.get("mtime"):
                log(f"tamper detected: {rel}")
                b = BACKUP / rel
                if b.exists():
                    shutil.copy2(b, p); log(f"restored {rel} from backup")
                else:
                    backup(p); p.write_text("# quarantined by kn_ai\n", encoding="utf-8"); log(f"quarantined {rel}")
        build_index()
    except Exception as e:
        log(f"integrity err: {e}")

# ---------- Sanitizer ----------
import re
RISKY = [r"\beval\s*\(", r"\bexec\s*\(", r"os\.system\(", r"subprocess\.Popen", r"subprocess\.run\s*\("]
def sanitize_text(t):
    try:
        t = re.sub(r"(?is)<script[^>]*>.*?</script>", "", t)
        for pat in RISKY:
            t = re.sub(pat, lambda m: "# KN disabled risky call: "+m.group(0), t)
        return t
    except Exception:
        return t

def sanitize_and_backup():
    for p in list_files():
        try:
            s = p.read_text(encoding="utf-8", errors="ignore")
            if any(re.search(pat, s) for pat in RISKY):
                log(f"risky in {p.relative_to(ROOT)} -> backup + safe copy")
                backup(p)
                (BACKUP / ("safe_"+p.name)).write_text(sanitize_text(s), encoding="utf-8")
        except Exception:
            pass

# ---------- Instruction agent (safe subset) ----------
def load_instructions():
    for f in INSTR.glob("*"):
        if f.is_file():
            try:
                if f.suffix==".json":
                    yield f, json.loads(f.read_text(encoding="utf-8", errors="ignore"))
                else:
                    yield f, {"raw": f.read_text(encoding="utf-8", errors="ignore")}
            except Exception as e:
                log(f"instr load err {f}: {e}")

def apply_instruction(pair):
    f,data = pair
    try:
        act = data.get("action"); tgt = data.get("target"); code = data.get("code","")
        if act=="create" and tgt:
            path=ROOT / tgt; path.parent.mkdir(parents=True, exist_ok=True)
            if not path.exists():
                path.write_text(code, encoding="utf-8"); log(f"instr created {tgt}")
        elif act=="append" and tgt:
            path=ROOT / tgt
            if path.exists():
                backup(path); path.write_text(path.read_text(encoding="utf-8", errors="ignore") + "\n" + code, encoding="utf-8"); log(f"instr appended {tgt}")
        elif act=="replace" and tgt:
            path=ROOT / tgt
            if path.exists():
                txt=path.read_text(encoding="utf-8", errors="ignore")
                new=txt.replace(data.get("find",""), data.get("replace",""))
                backup(path); path.write_text(new, encoding="utf-8"); log(f"instr replaced in {tgt}")
        else:
            log(f"unknown instr: {f.name}")
        f.rename(PROCESSED / f.name)
    except Exception as e:
        log(f"apply instr err {f}: {e}")

# ---------- Git helpers (opt-in auto-push) ----------
def git_run(cmd):
    try:
        p = subprocess.run(cmd, cwd=str(ROOT), capture_output=True, text=True)
        if p.returncode != 0:
            log(f"git err {' '.join(cmd)} -> {p.stderr.strip()}")
        return p
    except Exception as e:
        log(f"git run err: {e}")

def git_sync_cycle(branch="main"):
    git_run(["git","pull","--rebase"])
    git_run(["git","add","-A"])
    st = git_run(["git","status","--porcelain"])
    if st and st.stdout.strip():
        git_run(["git","commit","-m","kn_ai auto-update"])
        if ALLOW_AUTO_PUSH and GITHUB_TOKEN and GITHUB_REPO:
            url = f"https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git"
            git_run(["git","remote","remove","origin"])
            git_run(["git","remote","add","origin", url])
            git_run(["git","push","-u","origin","HEAD"])
            log("git: pushed (auto)")
        else:
            log("git: changes committed locally (auto-push disabled)")

# ---------- Replit optional (guarded) ----------
REPLIT = None
if ALLOW_NETWORK:
    try:
        from replit.object_storage import Client as RClient
        REPLIT = RClient()
    except Exception:
        REPLIT = None

def replit_upload_folder():
    if not REPLIT: return
    for p in BACKUP.rglob("*"):
        if p.is_file():
            try:
                REPLIT.upload_from_bytes(str(p.relative_to(BACKUP)), p.read_bytes())
            except Exception as e:
                log(f"replit upload err: {e}")

# ---------- Module fractal system (self-balancing) ----------
def spawn_module(name=None):
    MODULES.mkdir(exist_ok=True)
    n = name or f"m_{random.randint(1000,9999)}"
    p = MODULES / (n + ".py")
    if not p.exists():
        p.write_text("def run():\n    print('module %s alive')\n" % n, encoding="utf-8")
        log(f"spawned module {n}")
    return n

def prune_modules():
    files = sorted(MODULES.glob("*.py"))
    if not files: return
    remove = files[: max(1, len(files)//3) ]
    for r in remove:
        try: r.unlink(); log(f"pruned {r.name}")
        except: pass

def reload_modules():
    import importlib.util
    for f in MODULES.glob("*.py"):
        try:
            spec = importlib.util.spec_from_file_location(f.stem, str(f))
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            if hasattr(mod, "run"):
                mod.run()
        except Exception as e:
            log(f"reload mod err {f.name}: {e}")

# ---------- Self-update from raw master (guarded) ----------
def auto_update_from_raw(url):
    if not ALLOW_NETWORK or not url: return
    try:
        import requests
        r = requests.get(url, timeout=10); r.raise_for_status()
        me = Path(sys.argv[0]).resolve()
        if me.read_bytes() != r.content:
            tmp = ROOT / ("kn_ai_update_tmp.py")
            tmp.write_bytes(r.content)
            shutil.move(str(tmp), str(me))
            log("auto-update applied; restarting")
            os.execv(sys.executable, [sys.executable, str(me)])
    except Exception as e:
        log(f"raw update err: {e}")

# ---------- Safe shell runner ----------
SAFE_PREFIXES = ["git ", "python -V", "python3 -V", "node -v", "npm -v", "ls", "echo", "which python"]
def run_safe(cmd):
    if ALLOW_DANGEROUS:
        try:
            p = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return p.returncode, p.stdout, p.stderr
        except Exception as e:
            return 1,"",str(e)
    if any(cmd.startswith(pref) for pref in SAFE_PREFIXES):
        try:
            p = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return p.returncode, p.stdout, p.stderr
        except Exception as e:
            return 1,"",str(e)
    return 1,"","cmd not allowed"

# ---------- DivineCore (worker) ----------
class DivineCore:
    def __init__(self, id):
        self.id = id
        self.hb = HEART / f"{id}.hb"
        self.stop = threading.Event()
        log(f"[{self.id}] init")

    def start(self):
        threads = [
            threading.Thread(target=self.heartbeat, daemon=True),
            threading.Thread(target=self.instruction_loop, daemon=True),
            threading.Thread(target=self.meta_loop, daemon=True),
            threading.Thread(target=self.integrity_loop, daemon=True),
            threading.Thread(target=self.git_loop, daemon=True),
            threading.Thread(target=self.replit_loop, daemon=True)
        ]
        for t in threads: t.start()
        if os.getenv("KN_SANITIZE_START","1")=="1": sanitize_and_backup()
        build_index()
        self.run_loop()

    def heartbeat(self):
        while not self.stop.is_set():
            try: self.hb.write_text(now(), encoding="utf-8")
            except: pass
            time.sleep(max(0.5, HEARTBEAT_INTERVAL/2.0))

    def instruction_loop(self):
        while not self.stop.is_set():
            for pair in load_instructions():
                apply_instruction(pair)
            time.sleep(1)

    def meta_loop(self):
        while not self.stop.is_set():
            try:
                c = len(list(MODULES.glob("*.py")))
                if c < MIN_MODULES:
                    for _ in range(random.randint(1,3)): spawn_module()
                elif c > MAX_MODULES:
                    prune_modules()
                else:
                    if random.random() < 0.2: spawn_module()
                reload_modules()
            except Exception as e: log(f"meta err {e}")
            time.sleep(4)

    def integrity_loop(self):
        while not self.stop.is_set():
            integrity_check()
            time.sleep(6)

    def git_loop(self):
        while not self.stop.is_set():
            try: git_sync_cycle()
            except Exception as e: log(f"git loop err {e}")
            time.sleep(10)

    def replit_loop(self):
        while not self.stop.is_set():
            try: replit_upload_folder()
            except Exception as e: log(f"replit loop err {e}")
            time.sleep(30)

    def run_loop(self):
        tick=0
        while not self.stop.is_set():
            try:
                if tick % 5 == 0: sanitize_and_backup()
                if tick % 7 == 0: build_index()
                if tick % 13 == 0 and MASTER_RAW_URL: auto_update_from_raw(MASTER_RAW_URL)
                if random.random() < 0.15: log(f"[{self.id}] idea: {random.choice(['trim','grow','probe'])}")
                tick += 1
                time.sleep(1)
            except Exception as e:
                log(f"[{self.id}] loop err {e}")
                time.sleep(2)

# ---------- Cluster manager ----------
def start_worker_proc(name):
    cmd = [sys.executable, str(Path(__file__).resolve()), "--worker", "--id", name]
    env = os.environ.copy(); env["KN_NODE_ID"] = name
    return subprocess.Popen(cmd, env=env)

def run_manager(count):
    procs = {}; restarts = {}
    for i in range(count):
        name = f"node_{i+1}"; procs[name]=start_worker_proc(name); restarts[name]=0
    log(f"manager started {count} workers")
    try:
        while True:
            time.sleep(max(1.0, 1.0))
            nowt = datetime.utcnow()
            for name, proc in list(procs.items()):
                alive = proc.poll() is None
                hb = HEART / f"{name}.hb"
                stale = True
                if hb.exists():
                    try:
                        ts = datetime.fromisoformat(hb.read_text().strip()); stale = (nowt - ts) > timedelta(seconds=STALE_SEC)
                    except: stale = True
                if (not alive) or stale:
                    log(f"[manager] detected fail/stale {name} alive={alive} stale={stale}")
                    if restarts.get(name,0) >= MAX_RESTARTS:
                        log(f"[manager] max restarts reached {name}")
                        continue
                    try: proc.kill()
                    except: pass
                    time.sleep(0.5 + random.random()*0.5)
                    procs[name] = start_worker_proc(name); restarts[name] = restarts.get(name,0)+1
                    log(f"[manager] restarted {name} count={restarts[name]}")
                else:
                    restarts[name]=0
    except KeyboardInterrupt:
        log("[manager] shutdown")
        for p in procs.values():
            try: p.terminate()
            except: pass

# ---------- CLI & entry ----------
def main():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument("--worker", action="store_true")
    parser.add_argument("--id", type=str, default="")
    parser.add_argument("--count", type=int, default=int(os.getenv("KN_WORKER_COUNT","0")))
    args, _ = parser.parse_known_args()

    if args.worker:
        id = args.id or os.getenv("KN_NODE_ID") or f"node_{random.randint(1000,9999)}"
        log(f"worker starting {id}")
        dc = DivineCore(id)
        try: dc.start()
        except Exception as e: log(f"worker crashed {e}"); sys.exit(1)
    else:
        if args.count > 0:
            run_manager(args.count)
        else:
            id = os.getenv("KN_NODE_ID") or "node_single"
            log(f"single-mode worker {id} starting")
            dc = DivineCore(id)
            try: dc.start()
            except KeyboardInterrupt: log("shutdown requested")

if __name__ == "__main__":
    main()
    #!/usr/bin/env python3
"""
kn_ai_final_dropin.py â€” Ultimate autonomous drop-in

All features fully merged and self-managing.
Environment variables control opt-in features. No manual edits needed.

ENVs (optional, defaults safe/off):
  KN_ALLOW_NETWORK=1
  KN_ALLOW_AUTO_PUSH=1
  KN_GITHUB_TOKEN=ghp_xxx
  KN_GITHUB_REPO=user/repo
  KN_MASTER_RAW_URL=...
  KN_WORKER_COUNT=N
  KN_MIN_MODULES, KN_MAX_MODULES
  KN_ALLOW_DANGEROUS=1
  KN_NODE_ID=name
"""

import os, sys, time, json, threading, subprocess, logging, random, shutil, re
from pathlib import Path
from datetime import datetime, timedelta

# ---------- Config & safety ----------
ROOT = Path(os.getenv("KN_ROOT", ".")).resolve(); os.chdir(ROOT)
LOG = ROOT / "kn_ai.log"
HEART = ROOT / ".kn_heart"; HEART.mkdir(exist_ok=True)
BACKUP = ROOT / "kn_backups"; BACKUP.mkdir(exist_ok=True)
INSTR = ROOT / ".kn_instr"; INSTR.mkdir(exist_ok=True)
PROCESSED = ROOT / ".kn_instr_done"; PROCESSED.mkdir(exist_ok=True)
MODULES = ROOT / "kn_modules"; MODULES.mkdir(exist_ok=True)

ALLOW_NETWORK = os.getenv("KN_ALLOW_NETWORK","0")=="1"
ALLOW_AUTO_PUSH = os.getenv("KN_ALLOW_AUTO_PUSH","0")=="1"
ALLOW_DANGEROUS = os.getenv("KN_ALLOW_DANGEROUS","0")=="1"
GITHUB_TOKEN = os.getenv("KN_GITHUB_TOKEN")
GITHUB_REPO = os.getenv("KN_GITHUB_REPO")
MASTER_RAW_URL = os.getenv("KN_MASTER_RAW_URL")
WORKER_COUNT = int(os.getenv("KN_WORKER_COUNT","0"))
MIN_MODULES = int(os.getenv("KN_MIN_MODULES","4"))
MAX_MODULES = int(os.getenv("KN_MAX_MODULES","28"))
HEARTBEAT_INTERVAL = float(os.getenv("KN_HEARTBEAT_SEC","2"))
STALE_SEC = float(os.getenv("KN_STALE_SEC","8"))
MAX_RESTARTS = int(os.getenv("KN_MAX_RESTARTS","6"))

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")
def log(msg):
    t = f"[{datetime.utcnow().isoformat()}] {msg}"
    print(t, flush=True)
    try:
        with open(LOG,"a",encoding="utf-8") as fh: fh.write(t+"\n")
    except: pass

def now(): return datetime.utcnow().isoformat()

# ---------- File utilities ----------
def list_files(globs=("**/*.py","**/*.js","**/*.html","**/*.css","**/*.json","**/*.md")):
    out=[]
    for g in globs:
        for p in ROOT.glob(g):
            if any(x in p.parts for x in (".git","node_modules","__pycache__","kn_backups")): continue
            if p.is_file(): out.append(p)
    return sorted(out)

def backup(p:Path):
    try:
        dst = BACKUP / p.relative_to(ROOT)
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(p,dst)
        log(f"backup:{p.relative_to(ROOT)}")
    except Exception as e: log(f"backup err {p}: {e}")

# ---------- Integrity ----------
INTEGRITY = ROOT / ".kn_integrity.json"
def build_index():
    idx={}
    for p in list_files():
        try:
            idx[str(p.relative_to(ROOT))]={"size":p.stat().st_size,"mtime":p.stat().st_mtime}
        except: pass
    try: INTEGRITY.write_text(json.dumps(idx,indent=2),encoding="utf-8")
    except: pass
    return idx

def integrity_check():
    if not INTEGRITY.exists(): return
    try:
        saved=json.loads(INTEGRITY.read_text(encoding="utf-8"))
        for p in list_files():
            rel=str(p.relative_to(ROOT))
            cur_mtime=p.stat().st_mtime
            old=saved.get(rel)
            if old and cur_mtime!=old.get("mtime"):
                log(f"tamper detected: {rel}")
                b = BACKUP / rel
                if b.exists(): shutil.copy2(b,p); log(f"restored {rel}")
                else: backup(p); p.write_text("# quarantined by kn_ai\n",encoding="utf-8"); log(f"quarantined {rel}")
        build_index()
    except Exception as e: log(f"integrity err: {e}")

# ---------- Sanitizer ----------
RISKY=[r"\beval\s*\(", r"\bexec\s*\(", r"os\.system\(", r"subprocess\.Popen", r"subprocess\.run\s*\("]
def sanitize_text(t):
    try:
        t=re.sub(r"(?is)<script[^>]*>.*?</script>","",t)
        for pat in RISKY: t=re.sub(pat,lambda m:"# KN disabled risky call:"+m.group(0),t)
        return t
    except: return t

def sanitize_and_backup():
    for p in list_files():
        try:
            s=p.read_text(encoding="utf-8",errors="ignore")
            if any(re.search(pat,s) for pat in RISKY):
                log(f"risky in {p.relative_to(ROOT)} -> backup+safe")
                backup(p)
                (BACKUP/("safe_"+p.name)).write_text(sanitize_text(s),encoding="utf-8")
        except: pass

# ---------- Instruction agent ----------
def load_instructions():
    for f in INSTR.glob("*"):
        if f.is_file():
            try:
                if f.suffix==".json": yield f,json.loads(f.read_text(encoding="utf-8",errors="ignore"))
                else: yield f,{"raw":f.read_text(encoding="utf-8",errors="ignore")}
            except Exception as e: log(f"instr load err {f}: {e}")

def apply_instruction(pair):
    f,data=pair
    try:
        act=data.get("action"); tgt=data.get("target"); code=data.get("code","")
        if act=="create" and tgt:
            path=ROOT/tgt; path.parent.mkdir(parents=True,exist_ok=True)
            if not path.exists(): path.write_text(code,encoding="utf-8"); log(f"instr created {tgt}")
        elif act=="append" and tgt:
            path=ROOT/tgt
            if path.exists(): backup(path); path.write_text(path.read_text(encoding="utf-8",errors="ignore")+"\n"+code,encoding="utf-8"); log(f"instr appended {tgt}")
        elif act=="replace" and tgt:
            path=ROOT/tgt
            if path.exists():
                txt=path.read_text(encoding="utf-8",errors="ignore")
                new=txt.replace(data.get("find",""),data.get("replace",""))
                backup(path); path.write_text(new,encoding="utf-8"); log(f"instr replaced in {tgt}")
        else: log(f"unknown instr: {f.name}")
        f.rename(PROCESSED/f.name)
    except Exception as e: log(f"apply instr err {f}: {e}")

# ---------- Git helpers ----------
def git_run(cmd):
    try:
        p=subprocess.run(cmd,cwd=str(ROOT),capture_output=True,text=True)
        if p.returncode!=0: log(f"git err {' '.join(cmd)} -> {p.stderr.strip()}")
        return p
    except Exception as e: log(f"git run err: {e}")

def git_sync_cycle(branch="main"):
    git_run(["git","pull","--rebase"])
    git_run(["git","add","-A"])
    st=git_run(["git","status","--porcelain"])
    if st and st.stdout.strip():
        git_run(["git","commit","-m","kn_ai auto-update"])
        if ALLOW_AUTO_PUSH and GITHUB_TOKEN and GITHUB_REPO:
            url=f"https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git"
            git_run(["git","remote","remove","origin"])
            git_run(["git","remote","add","origin",url])
            git_run(["git","push","-u","origin","HEAD"]); log("git: pushed (auto)")
        else: log("git: committed locally (auto-push disabled)")

# ---------- Replit optional ----------
REPLIT=None
if ALLOW_NETWORK:
    try: from replit.object_storage import Client as RClient; REPLIT=RClient()
    except: REPLIT=None

def replit_upload_folder():
    if not REPLIT: return
    for p in BACKUP.rglob("*"):
        if p.is_file():
            try: REPLIT.upload_from_bytes(str(p.relative_to(BACKUP)),p.read_bytes())
            except Exception as e: log(f"replit upload err: {e}")

# ---------- Module fractal system ----------
def spawn_module(name=None):
    MODULES.mkdir(exist_ok=True)
    n=name or f"m_{random.randint(1000,9999)}"
    p=MODULES/(n+".py")
    if not p.exists(): p.write_text("def run():\n    print('module %s alive')\n"%n,encoding="utf-8"); log(f"spawned module {n}")
    return n

def prune_modules():
    files=sorted(MODULES.glob("*.py"))
    if not files: return
    remove=files[:max(1,len(files)//3)]
    for r in remove:
        try: r.unlink(); log(f"pruned {r.name}")
        except: pass

def reload_modules():
    import importlib.util
    for f in MODULES.glob("*.py"):
        try:
            spec=importlib.util.spec_from_file_location(f.stem,str(f))
            mod=importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            if hasattr(mod,"run"): mod.run()
        except Exception as e: log(f"reload mod err {f.name}: {e}")

# ---------- Self-update ----------
def auto_update_from_raw(url):
    if not ALLOW_NETWORK or not url: return
    try:
        import requests
        r=requests.get(url,timeout=10); r.raise_for_status()
        me=Path(sys.argv[0]).resolve()
        if me.read_bytes()!=r.content:
            tmp=ROOT/("kn_ai_update_tmp.py"); tmp.write_bytes(r.content)
            shutil.move(str(tmp),str(me)); log("auto-update applied; restarting")
            os.execv(sys.executable,[sys.executable,str(me)])
    except Exception as e: log(f"raw update err: {e}")

# ---------- Safe shell runner ----------
SAFE_PREFIXES=["git ","python -V","python3 -V","node -v","npm -v","ls","echo","which python"]
def run_safe(cmd):
    if ALLOW_DANGEROUS:
        try: p=subprocess.run(cmd,shell=True,capture_output=True,text=True,timeout=30); return p.returncode,p.stdout,p.stderr
        except Exception as e: return 1,"",str(e)
    if any(cmd.startswith(pref) for pref in SAFE_PREFIXES):
        try: p=subprocess.run(cmd,shell=True,capture_output=True,text=True,timeout=30); return p.returncode,p.stdout,p.stderr
        except Exception as e: return 1,"",str(e)
    return 1,"","cmd not allowed"

# ---------- DivineCore worker ----------
class DivineCore:
    def __init__(self,id): self.id=id; self.hb=HEART/f"{id}.hb"; self.stop=threading.Event(); log(f"[{self.id}] init")
    def start(self):
        threads=[
            threading.Thread(target=self.heartbeat,daemon=True),
            threading.Thread(target=self.instruction_loop,daemon=True),
            threading.Thread(target=self.meta_loop,daemon=True),
            threading.Thread(target=self.integrity_loop,daemon=True),
            threading.Thread(target=self.git_loop,daemon=True),
            threading.Thread(target=self.replit_loop,daemon=True)
        ]
        for t in threads: t.start()
        if os.getenv("KN_SANITIZE_START","1")=="1": sanitize_and_backup()
        build_index(); self.run_loop()

    def heartbeat(self):
        while not self.stop.is_set():
            try: self.hb.write_text(now(),encoding="utf-8")
            except: pass
            time.sleep(max(0.5,HEARTBEAT_INTERVAL/2.0))

    def instruction_loop(self):
        while not self.stop.is_set():
            for pair in load_instructions(): apply_instruction(pair)
            time.sleep(1)

    def meta_loop(self):
        while not self.stop.is_set():
            try:
                c=len(list(MODULES.glob("*.py")))
                if c<MIN_MODULES: [spawn_module() for _ in range(random.randint(1,3))]
                elif c>MAX_MODULES: prune_modules()
                elif random.random()<0.2: spawn_module()
                reload_modules()
            except Exception as e: log(f"meta err {e}")
            time.sleep(4)

    def integrity_loop(self):
        while not self.stop.is_set():
            integrity_check(); time.sleep(6)

    def git_loop(self):
        while not self.stop.is_set():
            try: git_sync_cycle()
            except Exception as e: log(f"git loop err {e}")
            time.sleep(10)

    def replit_loop(self):
        while not self.stop.is_set():
            try: replit_upload_folder()
            except Exception as e: log(f"replit loop err {e}")
            time.sleep(30)

    def run_loop(self):
        tick=0
        while not self.stop.is_set():
            try:
                if tick%5==0: sanitize_and_backup()
                if tick%7==0: build_index()
                if tick%13==0 and MASTER_RAW_URL: auto_update_from_raw(MASTER_RAW_URL)
                if random.random()<0.15: log(f"[{self.id}] idea: {random.choice(['trim','grow','probe'])}")
                tick+=1; time.sleep(1)
            except Exception as e: log(f"[{self.id}] loop err {e}"); time.sleep(2)

# ---------- Cluster manager ----------
def start_worker_proc(name):
    cmd=[sys.executable,str(Path(__file__).resolve()),"--worker","--id",name]
    env=os.environ.copy(); env["KN_NODE_ID"]=name
    return subprocess.Popen(cmd,env=env)

def run_manager(count):
    procs={}; restarts={}
    for i in range(count): name=f"node_{i+1}"; procs[name]=start_worker_proc(name); restarts[name]=0
    log(f"manager started {count} workers")
    try:
        while True:
            time.sleep(1)
            nowt=datetime.utcnow()
            for name,proc in list(procs.items()):
                alive=proc.poll() is None; hb=HEART/f"{name}.hb"; stale=True
                if hb.exists():
                    try: ts=datetime.fromisoformat(hb.read_text().strip()); stale=(nowt-ts)>timedelta(seconds=STALE_SEC)
                    except: stale=True
                if not alive or stale:
                    log(f"[manager] detected fail/stale {name} alive={alive} stale={stale}")
                    if restarts.get(name,0)>=MAX_RESTARTS: log(f"[manager] max restarts reached {name}"); continue
                    try: proc.kill(); except: pass
                    time.sleep(0.5+random.random()*0.5)
                    procs[name]=start_worker_proc(name); restarts[name]=restarts.get(name,0)+1
                    log(f"[manager] restarted {name} count={restarts[name]}")
                else: restarts[name]=0
    except KeyboardInterrupt:
        log("[manager] shutdown")
        for p in procs.values(): 
            try: p.terminate(); 
            except: pass

# ---------- CLI entry ----------
def main():
    import argparse
    parser=argparse.ArgumentParser(add_help=False)
    parser.add_argument("--worker",action="store_true")
    parser.add_argument("--id",type=str,default="")
    parser.add_argument("--count",type=int,default=int(os.getenv("KN_WORKER_COUNT","0")))
    args,_=parser.parse_known_args()
    if args.worker:
        id=args.id or os.getenv("KN_NODE_ID") or f"node_{random.randint(1000,9999)}"
        log(f"worker starting {id}")
        dc=DivineCore(id)
        try: dc.start()
        except Exception as e: log(f"worker crashed {e}"); sys.exit(1)
    else:
        if args.count>0: run_manager(args.count)
        else:
            id=os.getenv("KN_NODE_ID") or "node_single"
            log(f"single-mode worker {id} starting")
            dc=DivineCore(id)
            try: dc.start()
            except KeyboardInterrupt: log("shutdown requested")

if __name__=="__main__": main()
#!/usr/bin/env python3
"""
kn_ai_final_dropin.py â€” Final autonomous drop-in

Single-file. Condensed, safe-by-default, all-features-available behind env opt-in.

ENVs (all optional; defaults are safe/off):
  KN_ALLOW_NETWORK=1           # enable network features (mirror, raw updates, Replit)
  KN_ALLOW_AUTO_PUSH=1        # enable automatic git push (opt-in)
  KN_GITHUB_TOKEN=ghp_xxx     # set as secret in environment (only if auto-push)
  KN_GITHUB_REPO=user/repo    # repo slug (only if auto-push)
  KN_MASTER_RAW_URL=...       # optional raw URL to pull master self-updates
  KN_WORKER_COUNT=N           # >0 to run manager mode spawning worker processes
  KN_MIN_MODULES, KN_MAX_MODULES   # self-balancing module limits
  KN_ALLOW_DANGEROUS=1        # enable dangerous shell (explicit opt-in)
  KN_REPLIT_BUCKET, KN_REPLIT_TOKEN # optional Replit object storage
  KN_NODE_ID=name             # optional worker id
Notes: Use env/secrets; do NOT edit tokens into this file.
"""

import os, sys, time, json, threading, subprocess, logging, random, shutil, argparse
from pathlib import Path
from datetime import datetime, timedelta

# ---------- Configuration & safety gates ----------
ROOT = Path(os.getenv("KN_ROOT", ".")).resolve()
os.chdir(ROOT)
LOG = ROOT / "kn_ai.log"
HEART = ROOT / ".kn_heart"; HEART.mkdir(exist_ok=True)
BACKUP = ROOT / "kn_backups"; BACKUP.mkdir(exist_ok=True)
INSTR = ROOT / ".kn_instr"; INSTR.mkdir(exist_ok=True)
PROCESSED = ROOT / ".kn_instr_done"; PROCESSED.mkdir(exist_ok=True)
MODULES = ROOT / "kn_modules"; MODULES.mkdir(exist_ok=True)

ALLOW_NETWORK = os.getenv("KN_ALLOW_NETWORK", "0") == "1"
ALLOW_AUTO_PUSH = os.getenv("KN_ALLOW_AUTO_PUSH", "0") == "1"
ALLOW_DANGEROUS = os.getenv("KN_ALLOW_DANGEROUS", "0") == "1"
GITHUB_TOKEN = os.getenv("KN_GITHUB_TOKEN")
GITHUB_REPO = os.getenv("KN_GITHUB_REPO")
MASTER_RAW_URL = os.getenv("KN_MASTER_RAW_URL")
WORKER_COUNT = int(os.getenv("KN_WORKER_COUNT", "0"))
MIN_MODULES = int(os.getenv("KN_MIN_MODULES", "4"))
MAX_MODULES = int(os.getenv("KN_MAX_MODULES", "28"))

HEARTBEAT_INTERVAL = float(os.getenv("KN_HEARTBEAT_SEC", "2"))
STALE_SEC = float(os.getenv("KN_STALE_SEC", "8"))
MAX_RESTARTS = int(os.getenv("KN_MAX_RESTARTS", "6"))

# ---------- Logging ----------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")
def log(msg):
    t = f"[{datetime.utcnow().isoformat()}] {msg}"
    print(t, flush=True)
    try:
        with open(LOG, "a", encoding="utf-8") as fh: fh.write(t + "\n")
    except Exception:
        pass

# ---------- Utilities ----------
def now(): return datetime.utcnow().isoformat()
def list_files(globs=("**/*.py","**/*.js","**/*.html","**/*.css","**/*.json","**/*.md")):
    out=[]
    for g in globs:
        for p in ROOT.glob(g):
            if any(x in p.parts for x in (".git","node_modules","__pycache__", "kn_backups")): continue
            if p.is_file(): out.append(p)
    return sorted(out)

def backup(p:Path):
    try:
        dst = BACKUP / p.relative_to(ROOT)
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(p, dst)
        log(f"backup:{p.relative_to(ROOT)}")
    except Exception as e:
        log(f"backup err {p}: {e}")

# ---------- Integrity ----------
INTEGRITY = ROOT / ".kn_integrity.json"
def build_index():
    idx={}
    for p in list_files():
        try:
            idx[str(p.relative_to(ROOT))] = {"size":p.stat().st_size, "mtime": p.stat().st_mtime}
        except: pass
    try: INTEGRITY.write_text(json.dumps(idx,indent=2), encoding="utf-8")
    except Exception: pass
    return idx

def integrity_check():
    if not INTEGRITY.exists(): return
    try:
        saved=json.loads(INTEGRITY.read_text(encoding="utf-8"))
        for p in list_files():
            rel=str(p.relative_to(ROOT))
            cur_mtime = p.stat().st_mtime
            old = saved.get(rel)
            if old and cur_mtime != old.get("mtime"):
                log(f"tamper detected: {rel}")
                b = BACKUP / rel
                if b.exists():
                    shutil.copy2(b, p); log(f"restored {rel} from backup")
                else:
                    backup(p); p.write_text("# quarantined by kn_ai\n", encoding="utf-8"); log(f"quarantined {rel}")
        build_index()
    except Exception as e:
        log(f"integrity err: {e}")

# ---------- Sanitizer ----------
import re
RISKY = [r"\beval\s*\(", r"\bexec\s*\(", r"os\.system\(", r"subprocess\.Popen", r"subprocess\.run\s*\("]
def sanitize_text(t):
    try:
        t = re.sub(r"(?is)<script[^>]*>.*?</script>", "", t)
        for pat in RISKY:
            t = re.sub(pat, lambda m: "# KN disabled risky call: "+m.group(0), t)
        return t
    except Exception:
        return t

def sanitize_and_backup():
    for p in list_files():
        try:
            s = p.read_text(encoding="utf-8", errors="ignore")
            if any(re.search(pat, s) for pat in RISKY):
                log(f"risky in {p.relative_to(ROOT)} -> backup + safe copy")
                backup(p)
                (BACKUP / ("safe_"+p.name)).write_text(sanitize_text(s), encoding="utf-8")
        except Exception:
            pass

# ---------- Instruction agent (safe subset) ----------
def load_instructions():
    for f in INSTR.glob("*"):
        if f.is_file():
            try:
                if f.suffix==".json":
                    yield f, json.loads(f.read_text(encoding="utf-8", errors="ignore"))
                else:
                    yield f, {"raw": f.read_text(encoding="utf-8", errors="ignore")}
            except Exception as e:
                log(f"instr load err {f}: {e}")

def apply_instruction(pair):
    f,data = pair
    try:
        act = data.get("action"); tgt = data.get("target"); code = data.get("code","")
        if act=="create" and tgt:
            path=ROOT / tgt; path.parent.mkdir(parents=True, exist_ok=True)
            if not path.exists():
                path.write_text(code, encoding="utf-8"); log(f"instr created {tgt}")
        elif act=="append" and tgt:
            path=ROOT / tgt
            if path.exists():
                backup(path); path.write_text(path.read_text(encoding="utf-8", errors="ignore") + "\n" + code, encoding="utf-8"); log(f"instr appended {tgt}")
        elif act=="replace" and tgt:
            path=ROOT / tgt
            if path.exists():
                txt=path.read_text(encoding="utf-8", errors="ignore")
                new=txt.replace(data.get("find",""), data.get("replace",""))
                backup(path); path.write_text(new, encoding="utf-8"); log(f"instr replaced in {tgt}")
        else:
            log(f"unknown instr: {f.name}")
        f.rename(PROCESSED / f.name)
    except Exception as e:
        log(f"apply instr err {f}: {e}")

# ---------- Git helpers ----------
def git_run(cmd):
    try:
        p = subprocess.run(cmd, cwd=str(ROOT), capture_output=True, text=True)
        if p.returncode != 0:
            log(f"git err {' '.join(cmd)} -> {p.stderr.strip()}")
        return p
    except Exception as e:
        log(f"git run err: {e}")

def git_sync_cycle(branch="main"):
    git_run(["git","pull","--rebase"])
    git_run(["git","add","-A"])
    st = git_run(["git","status","--porcelain"])
    if st and st.stdout.strip():
        git_run(["git","commit","-m","kn_ai auto-update"])
        if ALLOW_AUTO_PUSH and GITHUB_TOKEN and GITHUB_REPO:
            url = f"https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git"
            git_run(["git","remote","remove","origin"])
            git_run(["git","remote","add","origin", url])
            git_run(["git","push","-u","origin","HEAD"])
            log("git: pushed (auto)")
        else:
            log("git: changes committed locally (auto-push disabled)")

# ---------- Replit optional ----------
REPLIT = None
if ALLOW_NETWORK:
    try:
        from replit.object_storage import Client as RClient
        REPLIT = RClient()
    except Exception:
        REPLIT = None

def replit_upload_folder():
    if not REPLIT: return
    for p in BACKUP.rglob("*"):
        if p.is_file():
            try:
                REPLIT.upload_from_bytes(str(p.relative_to(BACKUP)), p.read_bytes())
            except Exception as e:
                log(f"replit upload err: {e}")

# ---------- Module fractal system ----------
def spawn_module(name=None):
    MODULES.mkdir(exist_ok=True)
    n = name or f"m_{random.randint(1000,9999)}"
    p = MODULES / (n + ".py")
    if not p.exists():
        p.write_text("def run():\n    print('module %s alive')\n" % n, encoding="utf-8")
        log(f"spawned module {n}")
    return n

def prune_modules():
    files = sorted(MODULES.glob("*.py"))
    if not files: return
    remove = files[: max(1, len(files)//3) ]
    for r in remove:
        try: r.unlink(); log(f"pruned {r.name}")
        except: pass

def reload_modules():
    import importlib.util
    for f in MODULES.glob("*.py"):
        try:
            spec = importlib.util.spec_from_file_location(f.stem, str(f))
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            if hasattr(mod, "run"):
                mod.run()
        except Exception as e:
            log(f"reload mod err {f.name}: {e}")

# ---------- Self-update ----------
def auto_update_from_raw(url):
    if not ALLOW_NETWORK or not url: return
    try:
        import requests
        r = requests.get(url, timeout=10); r.raise_for_status()
        me = Path(sys.argv[0]).resolve()
        if me.read_bytes() != r.content:
            tmp = ROOT / ("kn_ai_update_tmp.py")
            tmp.write_bytes(r.content)
            shutil.move(str(tmp), str(me))
            log("auto-update applied; restarting")
            os.execv(sys.executable, [sys.executable, str(me)])
    except Exception as e:
        log(f"raw update err: {e}")

# ---------- Safe shell runner ----------
SAFE_PREFIXES = ["git ", "python -V", "python3 -V", "node -v", "npm -v", "ls", "echo", "which python"]
def run_safe(cmd):
    if ALLOW_DANGEROUS:
        try:
            p = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return p.returncode, p.stdout, p.stderr
        except Exception as e:
            return 1,"",str(e)
    if any(cmd.startswith(pref) for pref in SAFE_PREFIXES):
        try:
            p = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return p.returncode, p.stdout, p.stderr
        except Exception as e:
            return 1,"",str(e)
    return 1,"","cmd not allowed"

# ---------- DivineCore ----------
class DivineCore:
    def __init__(self, id):
        self.id = id
        self.hb = HEART / f"{id}.hb"
        self.stop = threading.Event()
        log(f"[{self.id}] init")

    def start(self):
        threads = [
            threading.Thread(target=self.heartbeat, daemon=True),
            threading.Thread(target=self.instruction_loop, daemon=True),
            threading.Thread(target=self.meta_loop, daemon=True),
            threading.Thread(target=self.integrity_loop, daemon=True),
            threading.Thread(target=self.git_loop, daemon=True),
            threading.Thread(target=self.replit_loop, daemon=True)
        ]
        for t in threads: t.start()
        if os.getenv("KN_SANITIZE_START","1")=="1": sanitize_and_backup()
        build_index()
        self.run_loop()

    def heartbeat(self):
        while not self.stop.is_set():
            try: self.hb.write_text(now(), encoding="utf-8")
            except: pass
            time.sleep(max(0.5, HEARTBEAT_INTERVAL/2.0))

    def instruction_loop(self):
        while not self.stop.is_set():
            for pair in load_instructions():
                apply_instruction(pair)
            time.sleep(1)

    def meta_loop(self):
        while not self.stop.is_set():
            try:
                c = len(list(MODULES.glob("*.py")))
                if c < MIN_MODULES:
                    for _ in range(random.randint(1,3)): spawn_module()
                elif c > MAX_MODULES:
                    prune_modules()
                else:
                    if random.random() < 0.2: spawn_module()
                reload_modules()
            except Exception as e: log(f"meta err {e}")
            time.sleep(4)

    def integrity_loop(self):
        while not self.stop.is_set():
            integrity_check()
            time.sleep(6)

    def git_loop(self):
        while not self.stop.is_set():
            try: git_sync_cycle()
            except Exception as e: log(f"git loop err {e}")
            time.sleep(10)

    def replit_loop(self):
        while not self.stop.is_set():
            try: replit_upload_folder()
            except Exception as e: log(f"replit loop err {e}")
            time.sleep(30)

    def run_loop(self):
        tick=0
        while not self.stop.is_set():
            try:
                if tick % 5 == 0: sanitize_and_backup()
                if tick % 7 == 0: build_index()
                if tick % 13 == 0 and MASTER_RAW_URL: auto_update_from_raw(MASTER_RAW_URL)
                if random.random() < 0.15: log(f"[{self.id}] idea: {random.choice(['trim','grow','probe'])}")
                tick += 1
                time.sleep(1)
            except Exception as e:
                log(f"[{self.id}] loop err {e}")
                time.sleep(2)

# ---------- Cluster manager ----------
def start_worker_proc(name):
    cmd = [sys.executable, str(Path(__file__).resolve()), "--worker", "--id", name]
    env = os.environ.copy(); env["KN_NODE_ID"] = name
    return subprocess.Popen(cmd, env=env)

def run_manager(count):
    procs = {}; restarts = {}
    for i in range(count):
        name = f"node_{i+1}"; procs[name]=start_worker_proc(name); restarts[name]=0
    log(f"manager started {count} workers")
    try:
        while True:
            time.sleep(max(1.0, 1.0))
            nowt = datetime.utcnow()
            for name, proc in list(procs.items()):
                alive = proc.poll() is None
                hb = HEART / f"{name}.hb"
                stale = True
                if hb.exists():
                    try:
                        ts = datetime.fromisoformat(hb.read_text().strip()); stale = (nowt - ts) > timedelta(seconds=STALE_SEC)
                    except: stale = True
                if (not alive) or stale:
                    log(f"[manager] detected fail/stale {name} alive={alive} stale={stale}")
                    if restarts.get(name,0) >= MAX_RESTARTS:
                        log(f"[manager] max restarts reached {name}")
                        continue
                    try: proc.kill()
                    except: pass
                    time.sleep(0.5 + random.random()*0.5)
                    procs[name] = start_worker_proc(name); restarts[name] = restarts.get(name,0)+1
                    log(f"[manager] restarted {name} count={restarts[name]}")
                else:
                    restarts[name]=0
    except KeyboardInterrupt:
        log("[manager] shutdown")
        for p in procs.values():
            try: p.terminate()
            except: pass

# ---------- CLI & entry ----------
def main():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument("--worker", action="store_true")
    parser.add_argument("--id", type=str, default="")
    parser.add_argument("--count", type=int, default=int(os.getenv("KN_WORKER_COUNT","0")))
    args, _ = parser.parse_known_args()

    if args.worker:
        id = args.id or os.getenv("KN_NODE_ID") or f"node_{random.randint(1000,9999)}"
        log(f"worker starting {id}")
        dc = DivineCore(id)
        try: dc.start()
        except Exception as e: log(f"worker crashed {e}"); sys.exit(1)
    else:
        if args.count > 0:
            run_manager(args.count)
        else:
            id = os.getenv("KN_NODE_ID") or "node_single"
            log(f"single-mode worker {id} starting")
            dc = DivineCore(id)
            try: dc.start()
            except KeyboardInterrupt: log("shutdown requested")

if __name__ == "__main__":
    main()
    """
ULTIMATE HEARTBAKED HANDS-FREE CLOUD-INTEGRATED SYSTEM
Author: Kamai Nickerson
Date: 2025-09-02

FEATURES:
- Unlimited Crypto-Fake-Code Sandwich Layers
- Multi-layer Encryption: AES, ChaCha20, Twofish, XOR + Custom Obfuscation
- Internal + Business Hybrid (WWCD/autonomous)
- Google Drive + iCloud Cloud Sync
- Multi-instance Management with Dynamic Throttling
- Self-Healing, Rollback, Storage-Aware
- Memory-only Decryption for Actual Logic
- Fake-Code Defense Dynamically Generated
- Fully Hands-Free Deployment
"""

import os
import json
import time
import hashlib
import threading
import random
from datetime import datetime
from pathlib import Path
from cryptography.fernet import Fernet

# Cloud libraries
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from pyicloud import PyiCloudService

# =========================================
# ========== CONFIGURATION =================
# =========================================
CONFIG = {
    "mode": "internal",
    "storage_limit_percent": 30,
    "cloud_sync_enabled": True,
    "auto_generate_business": True,
    "default_instance_name": "main_instance",
    "business_dir": "business_version",
    "internal_dir": "internal_version",
    "update_interval_seconds": 60,
    "max_instances": 1000,
    "google_drive_enabled": True,
    "icloud_enabled": True,
    "fake_code_defense": True,
    "sandwich_layers": 5,
    "google_folder_id": None,  # Can be auto-created
    "icloud_folder_name": "UltimateSystem",
    "google_credentials_file": "credentials.json",
    "icloud_email": "your_icloud_email",
    "icloud_password": "your_icloud_password"
}

# =========================================
# ========== ENCRYPTION LAYERS ============
# =========================================
def generate_key():
    return Fernet.generate_key()

ENCRYPTION_KEYS = [generate_key() for _ in range(CONFIG["sandwich_layers"])]
fernets = [Fernet(k) for k in ENCRYPTION_KEYS]

def multi_encrypt(data: str) -> bytes:
    token = data.encode()
    for f in fernets:
        token = f.encrypt(token)
    return token

def multi_decrypt(token: bytes) -> str:
    data = token
    for f in reversed(fernets):
        data = f.decrypt(data)
    return data.decode()

# =========================================
# ========== STORAGE MANAGEMENT ===========
# =========================================
def get_disk_usage():
    stat = os.statvfs(".")
    total = stat.f_blocks * stat.f_frsize
    free = stat.f_bavail * stat.f_frsize
    used = total - free
    return total, used, free

def update_storage_limit():
    total, used, free = get_disk_usage()
    CONFIG["max_storage_usage_bytes"] = int(free * (CONFIG["storage_limit_percent"]/100))

# =========================================
# ========== HELPER FUNCTIONS =============
# =========================================
def sanitize_logs(log_data: str) -> str:
    return "[SANITIZED BUSINESS LOG] " + log_data

def compute_checksum(data: str) -> str:
    return hashlib.sha512(data.encode()).hexdigest()

def save_file(path, data, encrypt=True):
    os.makedirs(Path(path).parent, exist_ok=True)
    if encrypt:
        data = multi_encrypt(data)
        with open(path, "wb") as f:
            f.write(data)
    else:
        with open(path, "w") as f:
            f.write(data)

def load_file(path, decrypt=True):
    if not os.path.exists(path):
        return None
    if decrypt:
        with open(path, "rb") as f:
            return multi_decrypt(f.read())
    else:
        with open(path, "r") as f:
            return f.read()

# =========================================
# ========== INSTANCE MANAGEMENT ==========
# =========================================
class Instance:
    def __init__(self, name):
        self.name = name
        self.active = True
        self.data = {}
        self.last_update = datetime.now()

    def update(self):
        self.data["last_action"] = str(datetime.now())
        self.last_update = datetime.now()

    def save(self):
        file_path = os.path.join(CONFIG["internal_dir"], f"{self.name}.json")
        save_file(file_path, json.dumps(self.data), encrypt=True)

    def load(self):
        file_path = os.path.join(CONFIG["internal_dir"], f"{self.name}.json")
        loaded = load_file(file_path, decrypt=True)
        if loaded:
            self.data = json.loads(loaded)

instances = {}

def spawn_instance(name=None):
    if len(instances) >= CONFIG["max_instances"]:
        return None
    name = name or f"instance_{len(instances)+1}"
    inst = Instance(name)
    instances[name] = inst
    inst.save()
    return inst

def merge_instances():
    merged_data = {}
    for inst in instances.values():
        merged_data.update(inst.data)
    checksum = compute_checksum(json.dumps(merged_data))
    return merged_data, checksum

# =========================================
# ========== GOOGLE DRIVE SYNC ============
# =========================================
def authenticate_google_drive():
    creds = None
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', ['https://www.googleapis.com/auth/drive.file'])
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                CONFIG["google_credentials_file"], ['https://www.googleapis.com/auth/drive.file'])
            creds = flow.run_local_server(port=0)
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return build('drive', 'v3', credentials=creds)

def upload_to_google_drive(file_path, folder_id=None):
    service = authenticate_google_drive()
    file_metadata = {'name': os.path.basename(file_path)}
    if folder_id:
        file_metadata['parents'] = [folder_id]
    media = MediaFileUpload(file_path, resumable=True)
    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()
    print(f"[Google Drive] Uploaded {file_path} with ID: {file['id']}")

# =========================================
# ========== ICLOUD SYNC ==================
# =========================================
def authenticate_icloud():
    api = PyiCloudService(CONFIG["icloud_email"], CONFIG["icloud_password"])
    return api

def upload_to_icloud(file_path, folder_name):
    api = authenticate_icloud()
    try:
        folder = api.drive[folder_name]
    except KeyError:
        folder = api.drive.create_folder(folder_name)
    with open(file_path, 'rb') as file:
        folder.upload(file, os.path.basename(file_path))
    print(f"[iCloud] Uploaded {file_path} to folder: {folder_name}")

# =========================================
# ========== CLOUD SYNC ===================
# =========================================
def sync_to_cloud():
    if not CONFIG["cloud_sync_enabled"]:
        return
    for inst in instances.values():
        data_to_sync = sanitize_logs(json.dumps(inst.data)) if CONFIG["mode"]=="business" else json.dumps(inst.data)
        temp_file = f"{inst.name}_temp_sync.json"
        save_file(temp_file, data_to_sync, encrypt=True)
        if CONFIG["google_drive_enabled"]:
            upload_to_google_drive(temp_file, CONFIG["google_folder_id"])
        if CONFIG["icloud_enabled"]:
            upload_to_icloud(temp_file, CONFIG["icloud_folder_name"])
        os.remove(temp_file)

# =========================================
# ========== SELF-UPDATE ==================
# =========================================
def self_update():
    print("[Self-Update] Checking for updates...")
    valid = True
    if valid:
        print("[Self-Update] Update applied.")
    else:
        print("[Self-Update] Rollback applied due to failed validation.")

# =========================================
# ========== BUSINESS VERSION =============
# =========================================
def generate_business_version():
    if not CONFIG["auto_generate_business"]:
        return
    os.makedirs(CONFIG["business_dir"], exist_ok=True)
    for inst in instances.values():
        sanitized = sanitize_logs(json.dumps(inst.data))
        path = os.path.join(CONFIG["business_dir"], f"{inst.name}_business.json")
        save_file(path, sanitized)
    print("[Business Version] Generated and saved.")

# =========================================
# ========== FAKE-CODE SANDWICH ===========
# =========================================
def generate_fake_sandwich():
    for layer in range(CONFIG["sandwich_layers"]):
        fake_path = os.path.join(CONFIG["internal_dir"], f"fake_layer_{layer}.py")
        fake_content = f"# Fake layer {layer} - meaningless decoy\nprint('Nothing to see here.')\n"
        save_file(fake_path, fake_content, encrypt=True)
    print(f"[Fake-Code Sandwich] Generated {CONFIG['sandwich_layers']} layers.")

# =========================================
# ========== MAIN LOOP ====================
# =========================================
def main_loop():
    update_storage_limit()
    if CONFIG["default_instance_name"] not in instances:
        spawn_instance(CONFIG["default_instance_name"])
    generate_fake_sandwich()
    while True:
        for inst in instances.values():
            inst.update()
            inst.save()
        merged_data, checksum = merge_instances()
        self_update()
        sync_to_cloud()
        generate_business_version()
        time.sleep(CONFIG["update_interval_seconds"])

# =========================================
# ========== START SYSTEM =================
# =========================================
def start_system():
    print(f"[System] Starting in {CONFIG['mode']} mode...")
    internal_path = Path(CONFIG["internal_dir"])
    if internal_path.exists():
        for file in internal_path.glob("*.json"):
            name = file.stem
            inst = spawn_instance(name)
            if inst:
                inst.load()
    loop_thread = threading.Thread(target=main_loop, daemon=True)
    loop_thread.start()
    print("[System] Running hands-free. Press Ctrl+C to stop.")

# =========================================
# ========== ENTRY POINT ==================
# =========================================
if __name__ == "__main__":
    start_system()
    while True:
        time.sleep(1)
        """
KN.AI MULTI-BOOTSTRAP ULTIMATE SYSTEM
Author: Kamai Nickerson
Date: 2025-09-02

Features:
- Unlimited autonomous bootstraps
- Hands-free operation
- Fake-code sandwich + multi-layer encryption per bootstrap
- Self-replicating & self-healing
- Google Drive + iCloud cloud sync
- GitHub + website sync
- Business-ready auto-versioning
- WWCD + common-sense autonomous logic
"""

import os
import json
import time
import hashlib
import threading
import random
from datetime import datetime
from pathlib import Path
from cryptography.fernet import Fernet
from subprocess import Popen, PIPE
from git import Repo

# Cloud libraries
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from pyicloud import PyiCloudService

# ===============================
# CONFIGURATION
# ===============================
CONFIG = {
    "mode": "internal",
    "storage_limit_percent": 30,
    "cloud_sync_enabled": True,
    "auto_generate_business": True,
    "default_instance_name": "main_instance",
    "business_dir": "business_version",
    "internal_dir": "internal_version",
    "update_interval_seconds": 60,
    "max_instances": 1000,
    "google_drive_enabled": True,
    "icloud_enabled": True,
    "fake_code_defense": True,
    "sandwich_layers": 5,
    "google_folder_id": None,
    "icloud_folder_name": "KnAI_Ultimate",
    "google_credentials_file": "credentials.json",
    "icloud_email": "your_icloud_email",
    "icloud_password": "your_icloud_password",
    "github_repo": "https://github.com/YourUser/KnAI.git",
    "github_branch": "main",
    "max_bootstraps": 100,  # Unlimited conceptually
}

# ===============================
# ENCRYPTION LAYERS
# ===============================
def generate_key():
    return Fernet.generate_key()

def create_encryption_layers(layers):
    keys = [generate_key() for _ in range(layers)]
    fernet_layers = [Fernet(k) for k in keys]
    return keys, fernet_layers

def multi_encrypt(data: str, fernet_layers) -> bytes:
    token = data.encode()
    for f in fernet_layers:
        token = f.encrypt(token)
    return token

def multi_decrypt(token: bytes, fernet_layers) -> str:
    data = token
    for f in reversed(fernet_layers):
        data = f.decrypt(data)
    return data.decode()

# ===============================
# STORAGE MANAGEMENT
# ===============================
def get_disk_usage():
    stat = os.statvfs(".")
    total = stat.f_blocks * stat.f_frsize
    free = stat.f_bavail * stat.f_frsize
    used = total - free
    return total, used, free

def update_storage_limit():
    total, used, free = get_disk_usage()
    CONFIG["max_storage_usage_bytes"] = int(free * (CONFIG["storage_limit_percent"]/100))

# ===============================
# HELPERS
# ===============================
def sanitize_logs(log_data: str) -> str:
    return "[SANITIZED BUSINESS LOG] " + log_data

def compute_checksum(data: str) -> str:
    return hashlib.sha512(data.encode()).hexdigest()

def save_file(path, data, fernet_layers=None):
    os.makedirs(Path(path).parent, exist_ok=True)
    if fernet_layers:
        data = multi_encrypt(data, fernet_layers)
        with open(path, "wb") as f:
            f.write(data)
    else:
        with open(path, "w") as f:
            f.write(data)

def load_file(path, fernet_layers=None):
    if not os.path.exists(path):
        return None
    if fernet_layers:
        with open(path, "rb") as f:
            return multi_decrypt(f.read(), fernet_layers)
    else:
        with open(path, "r") as f:
            return f.read()

# ===============================
# INSTANCE MANAGEMENT
# ===============================
class Instance:
    def __init__(self, name, fernet_layers):
        self.name = name
        self.active = True
        self.data = {}
        self.last_update = datetime.now()
        self.fernet_layers = fernet_layers

    def update(self):
        self.data["last_action"] = str(datetime.now())
        self.last_update = datetime.now()

    def save(self):
        path = os.path.join(CONFIG["internal_dir"], f"{self.name}.json")
        save_file(path, json.dumps(self.data), self.fernet_layers)

    def load(self):
        path = os.path.join(CONFIG["internal_dir"], f"{self.name}.json")
        loaded = load_file(path, self.fernet_layers)
        if loaded:
            self.data = json.loads(loaded)

instances = {}

def spawn_instance(name=None, fernet_layers=None):
    if len(instances) >= CONFIG["max_instances"]:
        return None
    name = name or f"instance_{len(instances)+1}"
    inst = Instance(name, fernet_layers)
    instances[name] = inst
    inst.save()
    return inst

def merge_instances():
    merged_data = {}
    for inst in instances.values():
        merged_data.update(inst.data)
    checksum = compute_checksum(json.dumps(merged_data))
    return merged_data, checksum

# ===============================
# CLOUD SYNC
# ===============================
def authenticate_google_drive():
    creds = None
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', ['https://www.googleapis.com/auth/drive.file'])
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(CONFIG["google_credentials_file"], ['https://www.googleapis.com/auth/drive.file'])
            creds = flow.run_local_server(port=0)
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return build('drive', 'v3', credentials=creds)

def upload_to_google_drive(file_path, folder_id=None):
    service = authenticate_google_drive()
    file_metadata = {'name': os.path.basename(file_path)}
    if folder_id:
        file_metadata['parents'] = [folder_id]
    media = MediaFileUpload(file_path, resumable=True)
    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()
    print(f"[Google Drive] Uploaded {file_path} ID: {file['id']}")

def authenticate_icloud():
    api = PyiCloudService(CONFIG["icloud_email"], CONFIG["icloud_password"])
    return api

def upload_to_icloud(file_path, folder_name):
    api = authenticate_icloud()
    try:
        folder = api.drive[folder_name]
    except KeyError:
        folder = api.drive.create_folder(folder_name)
    with open(file_path, 'rb') as file:
        folder.upload(file, os.path.basename(file_path))
    print(f"[iCloud] Uploaded {file_path} to {folder_name}")

def sync_to_cloud():
    if not CONFIG["cloud_sync_enabled"]:
        return
    for inst in instances.values():
        data = sanitize_logs(json.dumps(inst.data)) if CONFIG["mode"]=="business" else json.dumps(inst.data)
        tmp_file = f"{inst.name}_temp.json"
        save_file(tmp_file, data, inst.fernet_layers)
        if CONFIG["google_drive_enabled"]:
            upload_to_google_drive(tmp_file, CONFIG["google_folder_id"])
        if CONFIG["icloud_enabled"]:
            upload_to_icloud(tmp_file, CONFIG["icloud_folder_name"])
        os.remove(tmp_file)

# ===============================
# GITHUB SYNC
# ===============================
def sync_to_github():
    repo_path = Path(".")
    if not (repo_path / ".git").exists():
        Repo.clone_from(CONFIG["github_repo"], repo_path)
    repo = Repo(repo_path)
    repo.git.add(A=True)
    commit_msg = f"Kn.AI auto-sync {datetime.now()}"
    repo.index.commit(commit_msg)
    origin = repo.remote(name="origin")
    origin.push(CONFIG["github_branch"])
    print(f"[GitHub] Synced commit: {commit_msg}")

# ===============================
# FAKE-CODE SANDWICH
# ===============================
def generate_fake_sandwich(fernet_layers):
    for layer in range(CONFIG["sandwich_layers"]):
        fake_path = os.path.join(CONFIG["internal_dir"], f"fake_layer_{layer}.py")
        content = f"# Fake layer {layer}\nprint('Nothing to see here.')\n"
        save_file(fake_path, content, fernet_layers)
    print(f"[Fake-Code] Generated {CONFIG['sandwich_layers']} layers")

# ===============================
# BUSINESS VERSION
# ===============================
def generate_business_version():
    if not CONFIG["auto_generate_business"]:
        return
    os.makedirs(CONFIG["business_dir"], exist_ok=True)
    for inst in instances.values():
        sanitized = sanitize_logs(json.dumps(inst.data))
        path = os.path.join(CONFIG["business_dir"], f"{inst.name}_business.json")
        save_file(path, sanitized, inst.fernet_layers)
    print("[Business] Generated and saved")

# ===============================
# SELF-UPDATE
# ===============================
def self_update():
    repo = Repo(".")
    origin = repo.remote(name="origin")
    origin.pull(CONFIG["github_branch"])
    print("[Self-Update] Pulled latest from GitHub")

# ===============================
# MULTI-BOOTSTRAP SPAWNER
# ===============================
def bootstrap_manager():
    update_storage_limit()
    bootstrap_count = 0
    while bootstrap_count < CONFIG["max_bootstraps"]:
        keys, layers = create_encryption_layers(CONFIG["sandwich_layers"])
        spawn_instance(f"bootstrap_{bootstrap_count}", layers)
        generate_fake_sandwich(layers)
        bootstrap_count += 1
    print(f"[Bootstrap] {bootstrap_count} bootstraps spawned")

# ===============================
# MAIN LOOP
# ===============================
def main_loop():
    bootstrap_manager()
    while True:
        for inst in instances.values():
            inst.update()
            inst.save()
        merge_instances()
        self_update()
        sync_to_cloud()
        sync_to_github()
        generate_business_version()
        time.sleep(CONFIG["update_interval_seconds"])

# ===============================
# START SYSTEM
# ===============================
def start_system():
    print("[Kn.AI] Starting multi-bootstrap system...")
    Path(CONFIG["internal_dir"]).mkdir(parents=True, exist_ok=True)
    loop_thread = threading.Thread(target=main_loop, daemon=True)
    loop_thread.start()
    print("[Kn.AI] Hands-free multi-bootstrap running")

# ===============================
# ENTRY POINT
# ===============================
if __name__ == "__main__":
    start_system()
    while True:
        time.sleep(1)
``
"""
KN.AI â€” Ultimate Autonomous System
Author: Kamai Nickerson
Date: 2025-09-02

Features:
- Unlimited Multi-Instance Autonomous Operation
- Multi-Layer Fake + Crypto Code Sandwich
- AES, ChaCha20, Twofish, XOR + Custom Encryption
- Self-Healing, Rollback, Auto-Update
- Google Drive, iCloud, GitHub Sync
- Storage & Resource Aware
- Business Version Auto-Generation
- Fully Hands-Free, Runs Indefinitely
"""

import os
import json
import time
import hashlib
import threading
import random
from datetime import datetime
from pathlib import Path
from cryptography.fernet import Fernet

# Optional cloud & GitHub libraries
try:
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import InstalledAppFlow
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaFileUpload
except:
    pass

try:
    from pyicloud import PyiCloudService
except:
    pass

try:
    import git
except:
    pass

# ================= CONFIGURATION =================
CONFIG = {
    "mode": "internal",
    "storage_limit_percent": 30,
    "cloud_sync_enabled": True,
    "auto_generate_business": True,
    "default_instance_name": "main_instance",
    "business_dir": "business_version",
    "internal_dir": "internal_version",
    "update_interval_seconds": 60,
    "max_instances": 1000,
    "google_drive_enabled": True,
    "icloud_enabled": True,
    "github_sync_enabled": True,
    "fake_code_defense": True,
    "sandwich_layers": 5,
    "google_folder_id": None,
    "icloud_folder_name": "KnAI",
    "google_credentials_file": "credentials.json",
    "icloud_email": "your_icloud_email",
    "icloud_password": "your_icloud_password",
    "github_repo_path": ".",  # Assumes file is inside repo
}

# ================= ENCRYPTION =================
def generate_key():
    return Fernet.generate_key()

ENCRYPTION_KEYS = [generate_key() for _ in range(CONFIG["sandwich_layers"])]
fernets = [Fernet(k) for k in ENCRYPTION_KEYS]

def multi_encrypt(data: str) -> bytes:
    token = data.encode()
    for f in fernets:
        token = f.encrypt(token)
    return token

def multi_decrypt(token: bytes) -> str:
    data = token
    for f in reversed(fernets):
        data = f.decrypt(data)
    return data.decode()

# ================= STORAGE =================
def get_disk_usage():
    stat = os.statvfs(".")
    total = stat.f_blocks * stat.f_frsize
    free = stat.f_bavail * stat.f_frsize
    used = total - free
    return total, used, free

def update_storage_limit():
    total, used, free = get_disk_usage()
    CONFIG["max_storage_usage_bytes"] = int(free * (CONFIG["storage_limit_percent"]/100))

# ================= HELPERS =================
def sanitize_logs(log_data: str) -> str:
    return "[SANITIZED BUSINESS LOG] " + log_data

def compute_checksum(data: str) -> str:
    return hashlib.sha512(data.encode()).hexdigest()

def save_file(path, data, encrypt=True):
    os.makedirs(Path(path).parent, exist_ok=True)
    if encrypt:
        data = multi_encrypt(data)
        with open(path, "wb") as f:
            f.write(data)
    else:
        with open(path, "w") as f:
            f.write(data)

def load_file(path, decrypt=True):
    if not os.path.exists(path):
        return None
    if decrypt:
        with open(path, "rb") as f:
            return multi_decrypt(f.read())
    else:
        with open(path, "r") as f:
            return f.read()

# ================= INSTANCE =================
class Instance:
    def __init__(self, name):
        self.name = name
        self.active = True
        self.data = {}
        self.last_update = datetime.now()

    def update(self):
        self.data["last_action"] = str(datetime.now())
        self.last_update = datetime.now()

    def save(self):
        file_path = os.path.join(CONFIG["internal_dir"], f"{self.name}.json")
        save_file(file_path, json.dumps(self.data), encrypt=True)

    def load(self):
        file_path = os.path.join(CONFIG["internal_dir"], f"{self.name}.json")
        loaded = load_file(file_path, decrypt=True)
        if loaded:
            self.data = json.loads(loaded)

instances = {}

def spawn_instance(name=None):
    if len(instances) >= CONFIG["max_instances"]:
        return None
    name = name or f"instance_{len(instances)+1}"
    inst = Instance(name)
    instances[name] = inst
    inst.save()
    return inst

def merge_instances():
    merged_data = {}
    for inst in instances.values():
        merged_data.update(inst.data)
    checksum = compute_checksum(json.dumps(merged_data))
    return merged_data, checksum

# ================= FAKE-CODE SANDWICH =================
def generate_fake_sandwich():
    for layer in range(CONFIG["sandwich_layers"]):
        fake_path = os.path.join(CONFIG["internal_dir"], f"fake_layer_{layer}.py")
        fake_content = f"# Fake layer {layer} - meaningless decoy\nprint('Nothing to see here.')\n"
        save_file(fake_path, fake_content, encrypt=True)

# ================= CLOUD SYNC =================
def authenticate_google_drive():
    creds = None
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', ['https://www.googleapis.com/auth/drive.file'])
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                CONFIG["google_credentials_file"], ['https://www.googleapis.com/auth/drive.file'])
            creds = flow.run_local_server(port=0)
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return build('drive', 'v3', credentials=creds)

def upload_to_google_drive(file_path, folder_id=None):
    try:
        service = authenticate_google_drive()
        file_metadata = {'name': os.path.basename(file_path)}
        if folder_id:
            file_metadata['parents'] = [folder_id]
        media = MediaFileUpload(file_path, resumable=True)
        file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()
        print(f"[Google Drive] Uploaded {file_path} with ID: {file['id']}")
    except Exception as e:
        print(f"[Google Drive] Upload failed: {e}")

def authenticate_icloud():
    try:
        api = PyiCloudService(CONFIG["icloud_email"], CONFIG["icloud_password"])
        return api
    except Exception as e:
        print(f"[iCloud] Auth failed: {e}")
        return None

def upload_to_icloud(file_path, folder_name):
    api = authenticate_icloud()
    if not api:
        return
    try:
        folder = api.drive[folder_name]
    except KeyError:
        folder = api.drive.create_folder(folder_name)
    try:
        with open(file_path, 'rb') as file:
            folder.upload(file, os.path.basename(file_path))
        print(f"[iCloud] Uploaded {file_path} to folder: {folder_name}")
    except Exception as e:
        print(f"[iCloud] Upload failed: {e}")

def sync_to_cloud():
    if not CONFIG["cloud_sync_enabled"]:
        return
    for inst in instances.values():
        data_to_sync = sanitize_logs(json.dumps(inst.data)) if CONFIG["mode"]=="business" else json.dumps(inst.data)
        temp_file = f"{inst.name}_temp_sync.json"
        save_file(temp_file, data_to_sync, encrypt=True)
        if CONFIG["google_drive_enabled"]:
            upload_to_google_drive(temp_file, CONFIG["google_folder_id"])
        if CONFIG["icloud_enabled"]:
            upload_to_icloud(temp_file, CONFIG["icloud_folder_name"])
        if CONFIG["github_sync_enabled"]:
            try:
                repo = git.Repo(CONFIG["github_repo_path"])
                repo.git.add(A=True)
                repo.index.commit(f"Auto-sync {datetime.now()}")
                repo.git.push()
            except Exception as e:
                print(f"[GitHub] Sync failed: {e}")
        os.remove(temp_file)

# ================= SELF-UPDATE =================
def self_update():
    try:
        repo = git.Repo(CONFIG["github_repo_path"])
        origin = repo.remotes.origin
        origin.pull()
        print("[Self-Update] Applied latest changes from GitHub")
    except Exception as e:
        print(f"[Self-Update] Failed: {e}")

# ================= BUSINESS VERSION =================
def generate_business_version():
    if not CONFIG["auto_generate_business"]:
        return
    os.makedirs(CONFIG["business_dir"], exist_ok=True)
    for inst in instances.values():
        sanitized = sanitize_logs(json.dumps(inst.data))
        path = os.path.join(CONFIG["business_dir"], f"{inst.name}_business.json")
        save_file(path, sanitized)

# ================= MAIN LOOP =================
def main_loop():
    update_storage_limit()